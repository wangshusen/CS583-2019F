<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
  /*
   * Copyright 2013 Christophe-Marie Duquesne <chmd@chmd.fr>
   *
   * CSS for making a resume with pandoc. Inspired by moderncv.
   *
   * This CSS document is delivered to you under the CC BY-SA 3.0 License.
   * https://creativecommons.org/licenses/by-sa/3.0/deed.en_US
   */
  
  /* Whole document */
  body {
      font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
      width: 800px;
      margin: auto;
      background: #FFFFFF;
      padding: 10px 10px 10px 10px;
  }
  
  /* Title of the resume */
  h1 {
      font-size: 55px;
      color: #757575;
      text-align:center;
      margin-bottom:15px;
  }
  h1:hover {
      background-color: #757575;
      color: #FFFFFF;
      text-shadow: 1px 1px 1px #333;
  }
  
  /* Titles of categories */
  h2 {
      /* This is called "sectioncolor" in the ConTeXt stylesheet. */
      color: #397249;
  }
  /* There is a bar just before each category */
  h2:before {
      content: "";
      display: inline-block;
      margin-right:1%;
      width: 16%;
      height: 10px;
      /* This is called "rulecolor" in the ConTeXt stylesheet. */
      background-color: #9CB770;
  }
  h2:hover {
      background-color: #397249;
      color: #FFFFFF;
      text-shadow: 1px 1px 1px #333;
  }
  
  /* Definitions */
  dt {
      float: left;
      clear: left;
      width: 17%;
      /*font-weight: bold;*/
  }
  dd {
      margin-left: 17%;
  }
  p {
      margin-top:0;
      margin-bottom:7px;
  }
  
  /* Blockquotes */
  blockquote {
      text-align: center
  }
  
  /* Links */
  a {
      text-decoration: none;
      color: #397249;
  }
  a:hover, a:active {
      background-color: #397249;
      color: #FFFFFF;
      text-decoration: none;
      text-shadow: 1px 1px 1px #333;
  }
  
  /* Horizontal separators */
  hr {
      color: #A6A6A6;
  }
  
  table {
      width: 100%;
  }
  </style>
</head>
<body>
<h1 id="cs583-deep-learning">CS583: Deep Learning</h1>
<blockquote>
<p>Instructor: Shusen Wang</p>
</blockquote>
<blockquote>
<p>TA: Yao Xiao</p>
</blockquote>
<h2 id="description">Description</h2>
<p><strong>Meeting Time:</strong></p>
<ul>
<li>Thursday, 6:30-9:00 PM, North Building 102</li>
</ul>
<p><strong>Office Hours:</strong></p>
<ul>
<li>Thursday, 3:00 - 5:00 PM, Gateway South 354</li>
</ul>
<p><strong>Contact the Instructor:</strong></p>
<ul>
<li><p>For questions regarding grading, talk to the instructor during office hours or send him emails.</p></li>
<li><p>For any other questions, come during the office hours; the instructor will NOT reply such emails.</p></li>
</ul>
<p><strong>Prerequisite:</strong></p>
<ul>
<li><p>Elementary linear algebra, e.g., matrix multiplication, eigenvalue decomposition, and matrix norms.</p></li>
<li><p>Elementary calculus, e.g., convex function, differentiation of scalar functions, first derivative, and second derivative.</p></li>
<li><p>Python programming (especially the Numpy library) and Jupyter Notebook.</p></li>
</ul>
<p><strong>Goal:</strong> This is a practical course; the students will be able to use DL methods for solving real-world ML, CV, and NLP problems. The students will also learn math and theories for understanding ML and DL.</p>
<h2 id="schedule">Schedule</h2>
<ul>
<li><p>Aug 29, Preparations</p>
<ul>
<li><p>Install the software packages by following [<a href="https://github.com/wangshusen/CS583-2019F/blob/master/homework/Prepare/HM.pdf">this</a>]</p></li>
<li><p>Study elementary matrix algebra by following [<a href="http://vmls-book.stanford.edu/vmls.pdf">this book</a>]</p></li>
<li><p>Finish the [<a href="https://github.com/wangshusen/CS583-2019F/blob/master/homework/Quiz1-Sample/Q1.pdf">sample questions</a>] before Quiz 1.</p></li>
</ul></li>
<li><p>Aug 29, Lecture 1</p>
<ul>
<li><p>Fundamental ML problems</p></li>
<li><p>Regression</p></li>
</ul></li>
<li><p>Sep 5, <strong>Quiz 1</strong> after the lecture (around 5% of the total)</p>
<ul>
<li><p>Coverage: vectors norms (<span class="math inline">ℓ<sub>2</sub></span>-norm, <span class="math inline">ℓ<sub>1</sub></span>-norm, <span class="math inline">ℓ<sub><em>p</em></sub></span>-norm, <span class="math inline">ℓ<sub>∞</sub></span>-norm), vector inner product, matrix multiplication, matrix trace, matrix Frobenius norm, scalar function differential, convex function, use Numpy to construct vectors and matrices.</p></li>
<li><p>Policy: Printed material is allowed. No electronic device (except for electronic calculator).</p></li>
</ul></li>
<li><p>Sep 5, Lecture 2</p>
<ul>
<li><p>Read these before coming: [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/reading/MatrixCalculus.pdf">Matrix Calculus</a>][<a href="https://github.com/wangshusen/DeepLearning/blob/master/LectureNotes/Logistic/paper/logistic.pdf">Logistic Regression</a>]</p></li>
<li><p>Regression (Cont.)</p></li>
<li><p>Classification: logistic regression.</p></li>
</ul></li>
<li><p>Sep 12, Lecture 3</p>
<ul>
<li>Classification: SVM, softmax classifier, and KNN.</li>
</ul></li>
<li><p>Sep 19, Lecture 4</p>
<ul>
<li><p>Read Sections 1 to 3 before coming: [<a href="https://github.com/wangshusen/DeepLearning/blob/master/LectureNotes/SVD/svd.pdf">SVD and PCA</a>]</p></li>
<li><p>Read Sections 1 to 4 before coming: [<a href="https://github.com/wangshusen/DeepLearning/blob/master/LectureNotes/BP/bp.pdf">neural networks and backpropagation</a>]</p></li>
<li><p>Regularization.</p></li>
<li><p>Singular value decomposition (SVD).</p></li>
<li><p>Scientific computing libraries.</p></li>
<li><p>Neural networks.</p></li>
</ul></li>
<li><p>Sep 26, Lecture 5</p>
<ul>
<li><p>Keras.</p></li>
<li><p>Convolutional neural networks (CNNs).</p></li>
</ul></li>
<li><p>Oct 3, Lecture 6</p>
<ul>
<li><p>Finish reading the note before coming: [<a href="https://github.com/wangshusen/DeepLearning/blob/master/LectureNotes/BP/bp.pdf">neural networks and backpropagation</a>]</p></li>
<li><p>CNNs (Cont.)</p></li>
</ul></li>
<li><p>Oct 10, Lecture 7</p>
<ul>
<li><p>CNNs (Cont.)</p></li>
<li><p>Parallel computing.</p></li>
<li><p>Finish reading the note before coming: [<a href="https://github.com/wangshusen/DeepLearning/blob/master/LectureNotes/Parallel/Parallel.pdf">parallel computing</a>]</p></li>
</ul></li>
<li><p>Oct 17, Lecture 8</p>
<ul>
<li>Recurrent neural networks (RNNs).</li>
</ul></li>
<li><p>Oct 24, Lecture 9</p>
<ul>
<li>Quiz 2</li>
</ul></li>
<li><p>Oct 31, Lecture 10</p>
<ul>
<li><p>Text Generation.</p></li>
<li><p>Machine translation.</p></li>
</ul></li>
<li><p>Nov 7, Lecture 11</p>
<ul>
<li><p>Attention.</p></li>
<li><p>Transformer.</p></li>
<li><p>Autoencoders.</p></li>
</ul></li>
<li><p>Nov 14, Lecture 12</p>
<ul>
<li><p>Adversarial robustness.</p></li>
<li><p>GANs.</p></li>
<li><p>Variational autoencoder (VAE).</p></li>
</ul></li>
<li><p>Nov 21, Lecture 13</p>
<ul>
<li><p>Preview the slides before coming: [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/13_RL_1.pdf">slides</a>]</p></li>
<li><p>Deep reinforcement learning.</p></li>
<li><p>Collaborative filtering (optional).</p></li>
</ul></li>
<li><p>Dec 5, <strong>Final Exam</strong></p></li>
<li><p>Dec 12, Selected Project Presentation</p></li>
</ul>
<h2 id="assignments-and-bonus-scores">Assignments and Bonus Scores</h2>
<ul>
<li><p>Course Project</p>
<ul>
<li><p>Submit a proposal to Canvas before Oct 20.</p></li>
<li><p>Submit everything to Canvas before Dec 1.</p></li>
</ul></li>
<li><p>Project Presentation</p>
<ul>
<li><p>Voluntary, up to 5 bonus scores.</p></li>
<li><p>Submit relevant information to Canvas before Dec 1.</p></li>
<li><p>Up to 7 teams will be selected.</p></li>
</ul></li>
<li><p>Homework 1: Linear Algebra Basics</p>
<ul>
<li><p>Available only on Canvas (auto-graded.)</p></li>
<li><p>Submit to Canvas before Sep 22.</p></li>
</ul></li>
<li><p>Homework 2: Machine Learning Basics</p>
<ul>
<li><p>Available only on Canvas (auto-graded.)</p></li>
<li><p>Submit to Canvas before Oct 6.</p></li>
</ul></li>
<li><p>Homework 3: Implement a Convolutional Neural Network</p>
<ul>
<li><p>Available at the course's repo [<a href="https://github.com/wangshusen/CS583-2019F/tree/master/homework">click here</a>].</p></li>
<li><p>Submit to Canvas before Oct 27.</p></li>
</ul></li>
<li><p>Homework 4: Implement a Recurrent Neural Network</p>
<ul>
<li><p>Available at the course's repo [<a href="https://github.com/wangshusen/CS583-2019F/tree/master/homework">click here</a>].</p></li>
<li><p>Submit to Canvas before Nov 17.</p></li>
<li><p>You may get up to 3 bonus scores by doing extra work.</p></li>
</ul></li>
<li><p>Homework 5: Implement an Autoencoder Network</p>
<ul>
<li><p>Available at the course's repo [<a href="https://github.com/wangshusen/CS583-2019F/tree/master/homework">click here</a>].</p></li>
<li><p>Submit to Canvas before Dec 1.</p></li>
</ul></li>
<li><p>Bonus 1: Implement Numerical Optimization Algorithms (Voluntary)</p>
<ul>
<li><p>Available at the course's repo [<a href="https://github.com/wangshusen/CS583-2019F/tree/master/homework">click here</a>].</p></li>
<li><p>You will need the knowledge in the lecture note: [<a href="https://github.com/wangshusen/DeepLearning/blob/master/LectureNotes/Logistic/paper/logistic.pdf">Logistic Regression</a>]</p></li>
<li><p>Submit to Canvas before Oct 26 (firm deadline).</p></li>
<li><p>You may get up to 2 bonus scores.</p></li>
</ul></li>
<li><p>Bonus 2: Implement Numerical Optimization Algorithms (Voluntary)</p>
<ul>
<li><p>Available at Canvas.</p></li>
<li><p>You will need the knowledge in the lecture note: [<a href="https://github.com/wangshusen/DeepLearning/blob/master/LectureNotes/Parallel/Parallel.pdf">Parallel Computing</a>]</p></li>
<li><p>You can choose to implement Federated Averaging or/and Decentralized Optimization. You may get up to 2 bonus points for each.</p></li>
<li><p>Submit to Canvas before Nov 24 (firm deadline).</p></li>
</ul></li>
</ul>
<h2 id="syllabus-and-slides">Syllabus and Slides</h2>
<ol style="list-style-type: decimal">
<li><p><strong>Machine learning basics.</strong> This part briefly introduces the fundamental ML problems-- regression, classification, dimensionality reduction, and clustering-- and the traditional ML models and numerical algorithms for solving the problems.</p>
<ul>
<li><p>ML basics. [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/1_ML_Basics.pdf">slides-1</a>][<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/1_Models.pdf">slides-2</a>]</p></li>
<li><p>Regression. [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/2_Regression_1.pdf">slides-1</a>] [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/2_Regression_2.pdf">slides-2</a>]</p></li>
<li><p>Classification.</p>
<ul>
<li><p>Logistic regression: [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/3_Classification_1.pdf">slides</a>] [<a href="https://github.com/wangshusen/DeepLearning/blob/master/LectureNotes/Logistic/paper/logistic.pdf">lecture note</a>]</p></li>
<li><p>SVM: [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/3_Classification_2.pdf">slides</a>]</p></li>
<li><p>Softmax classifier: [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/3_Classification_3.pdf">slides</a>]</p></li>
<li><p>KNN classifier: [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/3_Classification_4.pdf">slides</a>]</p></li>
</ul></li>
<li><p>Regularizations. [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/3_Optimization.pdf">slides-1</a>][<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/3_Regularizations.pdf">slides-2</a>]</p></li>
<li><p>Clustering. [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/4_Clustering.pdf">slides</a>]</p></li>
<li><p>Dimensionality reduction. [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/5_DR_1.pdf">slides-1</a>] [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/5_DR_2.pdf">slides-2</a>] [<a href="https://github.com/wangshusen/DeepLearning/blob/master/LectureNotes/SVD/svd.pdf">lecture note</a>]</p></li>
<li><p>Scientific computing libraries. [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/5_DR_3.pdf">slides</a>]</p></li>
</ul></li>
<li><p><strong>Neural network basics.</strong> This part covers the multilayer perceptron, backpropagation, and deep learning libraries, with focus on Keras.</p>
<ul>
<li><p>Multilayer perceptron and backpropagation. [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/6_NeuralNet_1.pdf">slides</a>][<a href="https://github.com/wangshusen/DeepLearning/blob/master/LectureNotes/BP/bp.pdf">lecture note</a>]</p></li>
<li><p>Keras. [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/6_NeuralNet_2.pdf">slides</a>]</p></li>
<li><p>Further reading:</p>
<ul>
<li><p>[<a href="https://adl1995.github.io/an-overview-of-activation-functions-used-in-neural-networks.html">activation functions</a>]</p></li>
<li><p>[<a href="https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79">parameter initialization</a>]</p></li>
<li><p>[<a href="http://ruder.io/optimizing-gradient-descent/">optimization algorithms</a>]</p></li>
</ul></li>
</ul></li>
<li><p><strong>Convolutional neural networks (CNNs).</strong> This part is focused on CNNs and its application to computer vision problems.</p>
<ul>
<li><p>CNN basics. [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/7_CNN_1.pdf">slides</a>]</p></li>
<li><p>Tricks for improving test accuracy. [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/7_CNN_2.pdf">slides</a>]</p></li>
<li><p>Feature scaling and batch normalization. [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/7_CNN_3.pdf">slides</a>]</p></li>
<li><p>Advanced topics on CNNs. [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/7_CNN_4.pdf">slides</a>]</p></li>
<li><p>Popular CNN architectures. [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/7_CNN_5.pdf">slides</a>]</p></li>
<li><p>Face recognition. [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/7_CNN_6.pdf">slides</a>]</p></li>
<li><p>Further reading:</p>
<ul>
<li><p>[style transfer (Section 8.1, Chollet's book)]</p></li>
<li><p>[visualize CNN (Section 5.4, Chollet's book)]</p></li>
</ul></li>
</ul></li>
<li><p><strong>Autoencoders.</strong> This part introduces autoencoders for dimensionality reduction and image generation.</p>
<ul>
<li><p>Autoencoder for dimensionality reduction. [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/8_AE_1.pdf">slides</a>]</p></li>
<li><p>Variational Autoencoders (VAEs) for image generation. [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/8_AE_2.pdf">slides</a>]</p></li>
</ul></li>
<li><p><strong>Recurrent neural networks (RNNs).</strong> This part introduces RNNs and its applications in natural language processing (NLP).</p>
<ul>
<li><p>Text processing. [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/9_RNN_1.pdf">slides</a>]</p></li>
<li><p>RNN basics and LSTM. [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/9_RNN_2.pdf">slides</a>][<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">reference</a>]</p></li>
<li><p>Text generation. [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/9_RNN_3.pdf">slides</a>]</p></li>
<li><p>Machine translation. [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/9_RNN_4.pdf">slides</a>]</p></li>
<li><p>Image caption generation. [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/9_RNN_5.pdf">slides</a>][<a href="https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/">reference</a>]</p></li>
<li><p>Attention. [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/9_RNN_6.pdf">slides</a>][<a href="https://distill.pub/2016/augmented-rnns/">reference-1</a>] [<a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html">reference-2</a>]</p></li>
<li><p>Transformer model: beyond RNNs. [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/9_RNN_7.pdf">slides</a>][<a href="https://arxiv.org/pdf/1706.03762.pdf">reference</a>]</p></li>
<li><p>Further reading:</p>
<ul>
<li><p>[<a href="http://www.aclweb.org/anthology/D14-1162">GloVe: Global Vectors for Word Representation</a>]</p></li>
<li><p>[<a href="https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf">Neural Word Embedding as Implicit Matrix Factorization</a>]</p></li>
</ul></li>
</ul></li>
<li><p><strong>Recommender system.</strong> This part is focused on the collaborative filtering approach to recommendation based on the user-item rating data. This part covers matrix completion methods and neural network approaches.</p>
<ul>
<li>Collaborative filtering. [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/10_Recommender.pdf">slides</a>]</li>
</ul></li>
<li><p><strong>Adversarial Robustness.</strong> This part introduces how to attack neural networks using adversarial examples and how to defend from the attack.</p>
<ul>
<li><p>Data evasion attack and defense. [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/11_Adversarial.pdf">slides</a>][<a href="https://github.com/wangshusen/DeepLearning/blob/master/LectureNotes/Adversarial/DataAttacks.pdf">lecture note</a>]</p></li>
<li><p>Further reading: [<a href="https://adversarial-ml-tutorial.org/">Adversarial Robustness - Theory and Practice</a>]</p></li>
</ul></li>
<li><p><strong>Generative Adversarial Networks (GANs).</strong></p>
<ul>
<li>DC-GAN [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/12_GAN.pdf">slides</a>]</li>
</ul></li>
<li><p><strong>Deep Reinforcement Learning.</strong></p>
<ul>
<li><p>Reinforcement learning [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/13_RL_1.pdf">slides</a>]</p></li>
<li><p>Value-based learning [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/13_RL_2.pdf">slides</a>]</p></li>
<li><p>Policy-based learning [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/13_RL_3.pdf">slides</a>]</p></li>
<li><p>AlphaGo [<a href="https://github.com/wangshusen/DeepLearning/blob/master/Slides/13_RL_4.pdf">slides</a>]</p></li>
</ul></li>
</ol>
<h2 id="project">Project</h2>
<p>Every student need to participate in a <a href="https://www.kaggle.com/competitions">Kaggle competition</a>.</p>
<ul>
<li><p><strong>Details</strong>: [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/project/Project/proj.pdf">click here</a>] and download.</p></li>
<li><p><strong>Teamwork policy</strong>: You had better work on your own project. Teamwork (up to 3 students) is allowed if the competition has a heavy workload; the workload and team size will be considered in the grading.</p></li>
<li><p><strong>Grading policy</strong>: See the evaluation form [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/project/Evaluation/Evaluation.pdf">click here</a>]. An OK but not excellent work typically lose 3 points.</p></li>
</ul>
<p>Alternatively, one can work on any deep learning research project and submit a research paper style report. Note that the requirement is much higher than a Kaggle project:</p>
<ul>
<li><p>It will be evaluated as if it is a research paper submitted to ICML/CVPR/KDD. If it does not have sufficient novelty and technical contribution, it will receive a low score.</p></li>
<li><p>It cannot be a paper that published or posted on arXiv <strong>before</strong> the course begins. It is supposed done during this semester.</p></li>
</ul>
<h2 id="textbooks">Textbooks</h2>
<p><strong>Required</strong> (Please notice the difference between &quot;required&quot; and &quot;recommended&quot;):</p>
<ul>
<li>Francois Chollet. Deep learning with Python. Manning Publications Co., 2017. (Available online.)</li>
</ul>
<p><strong>Highly Recommended</strong>:</p>
<ul>
<li>S. Boyd and L. Vandenberghe. Introduction to Applied Linear Algebra. Cambridge University Press, 2018. (Available online.)</li>
</ul>
<p><strong>Recommended</strong>:</p>
<ul>
<li><p>Y. Nesterov. Introductory Lectures on Convex Optimization Book. Springer, 2013. (Available online.)</p></li>
<li><p>D. S. Watkins. Fundamentals of Matrix Computations. John Wiley &amp; Sons, 2004.</p></li>
<li><p>I. Goodfellow, Y. Bengio, A. Courville, Y. Bengio. Deep learning. MIT press, 2016. (Available online.)</p></li>
<li><p>M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of machine learning. MIT press, 2012.</p></li>
<li><p>J. Friedman, T. Hastie, and R. Tibshirani. The elements of statistical learning. Springer series in statistics, 2001. (Available online.)</p></li>
</ul>
<h2 id="grading-policy">Grading Policy</h2>
<p><strong>Weights</strong>:</p>
<ul>
<li><p>Homework 40%</p></li>
<li><p>Quizzes 20% (students' average score is likely around 17)</p></li>
<li><p>Final 20% (students' average score is likely around 16)</p></li>
<li><p>Project 20% (students' average score is likely around 17)</p></li>
<li><p>Bonus (up to 10%)</p></li>
</ul>
<p><strong>Expected grade on record</strong>:</p>
<ul>
<li><p>In the previous semester, the students' average scores in the quiz, final, and project are respectively 85%, 80%, and 85%.</p></li>
<li><p>Thus, an average student is expected to lose at least 3+4+3=10 points.</p></li>
<li><p>If an average student does not collect any bonus score, his grade on record is expected to be &quot;B+&quot;. An average student needs at least 3 bonus scores to get &quot;A&quot;.</p></li>
<li><p>According to Stevens's policy, a score lower than 73.0 will be fail.</p></li>
</ul>
<p><strong>Late penalty</strong>:</p>
<ul>
<li><p>Late submissions of assignments or project document for whatever reason will be punished. 2% of the score of an assignment/project will be deducted per day. For example, if an assignment is submitted 15 days and 1 minute later than the deadline (counted as 16 days) and it gets a grade of 95%, then the score after the deduction will be: 95% - 2*16% = 63%.</p></li>
<li><p>All the deadlines for bonus are firm. Late submission will not receive bonus score.</p></li>
<li><p>Dec 20 is the firm deadline for all the homework and the course project. Submissions later than the firm deadline will not be graded.</p></li>
</ul>
</body>
</html>
